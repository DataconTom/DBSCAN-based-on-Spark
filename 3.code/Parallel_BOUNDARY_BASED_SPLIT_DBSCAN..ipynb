{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rtree import index\n",
    "from collections import deque\n",
    "from queue import PriorityQueue\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Status\n",
    "UNKNOWN = -1\n",
    "NOISE = -2\n",
    "\n",
    "class DataLoader(object):\n",
    "    @staticmethod\n",
    "    def load_data_label(path: str):\n",
    "        \"\"\"\n",
    "        this is for input file with (coordinate_x, coordinate_y, ... , label) in each line\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = []\n",
    "            label = []\n",
    "            for l in f.readlines():\n",
    "                source = l.strip().split()\n",
    "                data.append([float(val) for val in source[:-1]])\n",
    "                label.append(int(source[-1]))\n",
    "            return np.array(data), np.array(label)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(path: str):\n",
    "        \"\"\"\n",
    "        this is for input file with (coordinate_x, coordinate_y, ...) in each line\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = []\n",
    "            for l in f.readlines():\n",
    "                source = l.strip().split()\n",
    "                data.append([float(val) for val in source])\n",
    "            return np.array(data)\n",
    "\n",
    "        \n",
    "class Evaluation(object):\n",
    "    @classmethod\n",
    "    def silhouette_coefficient(cls, dbscan_obj):\n",
    "        def a(pid, tags, dist_matrix):\n",
    "            mask = tags == tags[pid]\n",
    "            avg_dist = np.sum(dist_matrix[pid] * mask, axis=0) / np.sum(mask)\n",
    "            return avg_dist\n",
    "\n",
    "        def b(pid, tags, dist_matrix):\n",
    "            avg_dists = []\n",
    "            for label in range(1,\n",
    "                               max(tags) + 1):  # cluster label starts from 1\n",
    "                if label == tags[pid]:\n",
    "                    continue\n",
    "                mask = tags == label\n",
    "                avg_dists.append(\n",
    "                    np.sum(dist_matrix[pid] * mask, axis=0) / np.sum(mask))\n",
    "            return min(avg_dists)\n",
    "\n",
    "        # preparation\n",
    "        # if sum(dbscan_obj.tags) == -dbscan_obj.num_p:\n",
    "        if sum(dbscan_obj.tags) < 0:\n",
    "            raise Exception(f'eps:{dbscan_obj.eps} and min_pts:{dbscan_obj.min_pts} can cluter dataset well!')\n",
    "            \n",
    "        if not hasattr(dbscan_obj, 'dist_m'):\n",
    "            # by default, we try to use matrix dbscan to tune parameters\n",
    "            # BUG: If use basic dbscan has no _get_distance_matrix() attribute function\n",
    "            dbscan_obj._get_distance_matrix()\n",
    "        tags = np.array(dbscan_obj.tags)\n",
    "\n",
    "        # TODO: this method still can be optimised by matrix computation\n",
    "        res = 0\n",
    "        for pid in range(dbscan_obj.num_p):\n",
    "            tmp_a = a(pid, tags, dbscan_obj.dist_m)\n",
    "            tmp_b = b(pid, tags, dbscan_obj.dist_m)\n",
    "            res += (tmp_b - tmp_a) / max(tmp_b, tmp_a)\n",
    "        res /= dbscan_obj.num_p\n",
    "\n",
    "        print(\n",
    "            f'eps: {dbscan_obj.eps} min points: {dbscan_obj.min_pts} silhouette coefficient: {res}'\n",
    "        )\n",
    "        return res\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **wargs):\n",
    "        start = time.time()\n",
    "        res = func(*args, **wargs)\n",
    "        end = time.time()\n",
    "        print(f'{func.__name__} time cost: {(end-start)*1000}ms')\n",
    "        return res    \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DBSCAN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN(object):\n",
    "    \"\"\"\n",
    "    Base Class of DBSCAN, please do NOT instantiate this Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, eps, min_pts, metric='euclidian'):\n",
    "        \"\"\"\n",
    "        :param: dataset: numpy array or numpy matrix; set of data point coordinates \n",
    "        :param: eps: float; the value of radius of density area\n",
    "        :param: min_pts: int; least neighbours should be in a density area\n",
    "        :param: metric: str; the distance metric: (euclidian, manhattan, fast-euclidian)\n",
    "        \"\"\"\n",
    "        self.m, _ = (dataset, None\n",
    "                     )  # placeholder _ for future implementation of labels\n",
    "        self.num_p = self.m.shape[0]\n",
    "        self.tags = [UNKNOWN] * self.num_p\n",
    "        self.is_core = [0] * self.num_p\n",
    "\n",
    "        self.eps = eps**2 if metric == 'fast-euclidian' else eps\n",
    "        self.min_pts = min_pts\n",
    "        self.metric = metric\n",
    "\n",
    "    def _get_dist(self, a, b) -> float:\n",
    "        \"\"\"\n",
    "        for float comparison, set all distance value precision to 5\n",
    "        :param: a: int; index of given point in data matrix\n",
    "        :param: b: same as a\n",
    "        \"\"\"\n",
    "        if self.metric == 'euclidian':\n",
    "            result = np.sqrt(np.power(self.m[b] - self.m[a], 2).sum())\n",
    "        elif self.metric == 'manhattan':\n",
    "            result = np.abs(self.m[b] - self.m[a]).sum()\n",
    "        elif self.metric == 'fast-euclidian':\n",
    "            result = np.power(self.m[b] - self.m[a], 2).sum()\n",
    "        else:\n",
    "            raise Exception(f'Distance metric {self.metric} is invalid!')\n",
    "\n",
    "        return round(result, 5)\n",
    "\n",
    "    def _get_neighbours(self, p: int) -> list:\n",
    "        \"\"\"\n",
    "        return neighbours index of given point p in source data matrix\n",
    "        :param: p: int; index of given point in data matrix\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _clustering(self, p, cluster_id):\n",
    "        \"\"\"\n",
    "        tag given point p and all of its neighbours and sub-neighbours with the same cluster id\n",
    "        :param: m: np.matrix; N * 2 matrix recoding all nodes' coordinates\n",
    "        :param: cluster_id: int; current id of cluster\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _find_core_pts(self):\n",
    "        self.is_core = [0] * self.num_p\n",
    "        for i in range(self.num_p):\n",
    "            if len(self._get_neighbours(i)) > self.min_pts:\n",
    "                self.is_core[i] = 1\n",
    "        return self.is_core\n",
    "\n",
    "    @timeit\n",
    "    def predict(self) -> list:\n",
    "        \"\"\"\n",
    "        return list of labels as the sequence in data matrix\n",
    "        :param: m: np.matrix; N * 2 matrix recoding all nodes' coordinates\n",
    "        :param: eps: float; the value of radius of density area\n",
    "        :param: min_pts: int; least neighbours should be in a density area\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_id = 1\n",
    "        for p_id in range(self.num_p):\n",
    "            if self.tags[p_id] != UNKNOWN:\n",
    "                continue\n",
    "            if self._clustering(p_id, cluster_id):\n",
    "                cluster_id += 1\n",
    "        return np.array(self.tags)\n",
    "\n",
    "\n",
    "class NaiveDBSCAN(DBSCAN):\n",
    "    def __init__(self, dataset, eps, min_pts, metric='euclidian'):\n",
    "        super(NaiveDBSCAN, self).__init__(dataset, eps, min_pts, metric)\n",
    "\n",
    "    def _get_neighbours(self, p: int) -> list:\n",
    "\n",
    "        ngbs = []\n",
    "        for idx in range(len(self.m)):\n",
    "            if self._get_dist(p, idx) < self.eps:\n",
    "                ngbs.append(idx)\n",
    "        return ngbs\n",
    "\n",
    "    def _clustering(self, p, cluster_id) -> bool:\n",
    "\n",
    "        neighbours = self._get_neighbours(p)\n",
    "        if len(neighbours) < self.min_pts:\n",
    "            self.tags[p] = NOISE\n",
    "            return False\n",
    "        else:\n",
    "            self.tags[p] = cluster_id\n",
    "            for idx in neighbours:\n",
    "                self.tags[idx] = cluster_id\n",
    "            while len(neighbours) > 0:\n",
    "                sub_neighbours = self._get_neighbours(neighbours[0])\n",
    "                if len(sub_neighbours) >= self.min_pts:\n",
    "                    for sub_n in sub_neighbours:\n",
    "                        if self.tags[sub_n] < 0:\n",
    "                            self.tags[sub_n] = cluster_id\n",
    "                            if self.tags[sub_n] == UNKNOWN:\n",
    "                                neighbours.append(sub_n)\n",
    "                neighbours = neighbours[1:]\n",
    "        return True\n",
    "\n",
    "\n",
    "class MatrixDBSCAN(DBSCAN):\n",
    "    def __init__(self, dataset, eps, min_pts, metric='euclidian'):\n",
    "        super(MatrixDBSCAN, self).__init__(dataset, eps, min_pts, metric)\n",
    "\n",
    "    def _get_distance_matrix(self):\n",
    "        \"\"\"\n",
    "        Only once calculation will be on each point-pairs\n",
    "        results will be stored in self.dist_m\n",
    "        \"\"\"\n",
    "        self.dist_m = np.zeros((self.num_p, self.num_p))\n",
    "        for p_id in range(self.num_p):\n",
    "            for q_id in range(p_id, self.num_p):\n",
    "                dist = self._get_dist(p_id, q_id)\n",
    "                self.dist_m[q_id, p_id] = dist\n",
    "                self.dist_m[p_id, q_id] = dist\n",
    "\n",
    "    def _get_neighbours(self, p: int) -> list:\n",
    "        if not hasattr(self, 'dist_m'):                #判断里面是否有‘dist_m'\n",
    "            self._get_distance_matrix()\n",
    "        return np.nonzero(self.dist_m[p] < self.eps)[0] #判断有多少个dist 小于eps\n",
    "\n",
    "    def _clustering(self, p, cluster_id) -> bool:\n",
    "        \"\"\"\n",
    "        TODO: There should be some optimizations for this part, current code is too ugly\n",
    "        \"\"\"\n",
    "\n",
    "        neighbours = self._get_neighbours(p)\n",
    "        if len(neighbours) < self.min_pts:\n",
    "            self.tags[p] = NOISE\n",
    "            return False\n",
    "        else:\n",
    "            self.tags[p] = cluster_id\n",
    "            for idx in neighbours:\n",
    "                self.tags[idx] = cluster_id\n",
    "            while len(neighbours) > 0:\n",
    "                sub_neighbours = self._get_neighbours(neighbours[0])\n",
    "                if len(sub_neighbours) >= self.min_pts:\n",
    "                    for sub_n in sub_neighbours:\n",
    "                        if self.tags[sub_n] < 0:\n",
    "                            self.tags[sub_n] = cluster_id\n",
    "                            if self.tags[sub_n] == UNKNOWN:\n",
    "                                neighbours.append(sub_n)\n",
    "                neighbours = neighbours[1:]\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "SparkContext.getOrCreate()\n",
    "SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  General DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_label(path):\n",
    "#Strip () is used to remove the preceding space, and then take the last column as the label\n",
    "#Change each x element to a float tuple and save it\n",
    "    pts = sc.textFile(path).map(lambda x: x.strip().split()[:-1]).map(lambda x: tuple([float(i) for i in x]))\n",
    "    return pts.collect()\n",
    "\n",
    "def load_data(path):\n",
    "#Strip () is used to remove the preceding space, and then take all the columns as the dataset\n",
    "#Change each x element to a float tuple and save it\n",
    "    pts = sc.textFile(path).map(lambda x: x.strip().split()).map(lambda x: tuple([float(i) for i in x]))\n",
    "    return pts.collect()\n",
    "\n",
    "def _bounds_coordinates(bin_bounds):\n",
    "\n",
    "    lower_cdnts = [[low] for low in  bin_bounds[0][:-1]]\n",
    "    upper_cdnts = [[high] for high in bin_bounds[0][1:]]\n",
    "    \n",
    "    # super stupid implementation, optimization needed\n",
    "    for bound in bin_bounds[1:]:\n",
    "        lower_tmp = []\n",
    "        upper_tmp = []\n",
    "        \n",
    "        for bc in bound[:-1]:\n",
    "            lower_tmp.extend([lc + [bc] for lc in lower_cdnts])\n",
    "        lower_cdnts = lower_tmp\n",
    "        \n",
    "        for bc in bound[1:]:\n",
    "            upper_tmp.extend([uc + [bc] for uc in upper_cdnts])\n",
    "        upper_cdnts = upper_tmp\n",
    "        \n",
    "    return np.array(lower_cdnts), np.array(upper_cdnts)\n",
    "\n",
    "@timeit\n",
    "def spatial_partition(dataset, n_partitions, eps):\n",
    "    \"\"\"\n",
    "    n_partitions: tuple with shape correspoding to dataset dimension\n",
    "    \"\"\"\n",
    "    tp_par = n_partitions\n",
    "    num_par = np.prod(n_partitions)  # prod(2，2)= 2*2 = 4个partition\n",
    "    \n",
    "    # cut bins\n",
    "    bounds = np.concatenate(([np.min(dataset, axis=0)], [np.max(dataset, axis=0)]), axis=0)     # 2 * D\n",
    "    bounds_dim = bounds.T   # D * 2, \n",
    "    print(\"tp_par:\" + tp_par)\n",
    "    print(\"bounds_dim:\" + bounds_dim)\n",
    "    \n",
    "    bin_bounds = []\n",
    "    for i in range(len(tp_par)):\n",
    "        dim_bins = np.linspace(*bounds_dim[i], tp_par[i]+1, endpoint=True)\n",
    "        bin_bounds.append(dim_bins)\n",
    "    print(\"bin_bounds:\" + bin_bounds)\n",
    "    \n",
    "    lower_bounds, upper_bounds = _bounds_coordinates(bin_bounds)\n",
    "    lower_bounds -= eps\n",
    "    upper_bounds += eps\n",
    "\n",
    "    # scatter points into bins with eps\n",
    "    indexed_data = []\n",
    "    for id_pts in range(len(dataset)):     # index of point in dataset\n",
    "        for id_ptt in range(num_par):\n",
    "            if not (dataset[id_pts] > lower_bounds[id_ptt]).all():\n",
    "                continue\n",
    "            if not (dataset[id_pts] < upper_bounds[id_ptt]).all():\n",
    "                continue\n",
    "            indexed_data.append([id_ptt, id_pts])\n",
    "            \n",
    "    res = sc.parallelize(indexed_data).groupByKey().map(lambda x: [x[0], list(x[1])])\n",
    "    return res\n",
    "\n",
    "def local_dbscan(partioned_rdd, method='matrix', metric='euclidian'):\n",
    "\n",
    "    dataset = np.array([b_dataset.value[idp] for idp in partioned_rdd])\n",
    "    if method == 'matrix':\n",
    "        dbscan_obj = MatrixDBSCAN(dataset, b_eps.value, b_min_pts.value, metric) \n",
    "    else:\n",
    "        dbscan_obj = NaiveDBSCAN(dataset, b_eps.value, b_min_pts.value, metric) \n",
    "    dbscan_obj.predict()\n",
    "    is_core_list = dbscan_obj._find_core_pts()\n",
    "    \n",
    "    return list(zip(zip(partioned_rdd, is_core_list), dbscan_obj.tags))\n",
    "\n",
    "@timeit\n",
    "def merge(local_tags, dataset):\n",
    "    global_tags = [UNKNOWN] * len(dataset)\n",
    "    is_tagged = [0] * len(dataset)\n",
    "    last_max_label = 0\n",
    "    for local in local_tags:\n",
    "        np_local = np.array(local[-1])\n",
    "        np_local[:, -1] += last_max_label\n",
    "\n",
    "        last_max_label = np.max(np_local[:, -1])\n",
    "        \n",
    "        # check and merge overlapped points\n",
    "        tagged_indices = np.nonzero(is_tagged)[0]\n",
    "        for tmp_i in range(len(np_local)):\n",
    "            # should do tag check\n",
    "            (p_id, is_core), label = np_local[tmp_i]\n",
    "            if p_id in tagged_indices and is_core==1:\n",
    "                np_local[-1][np_local[-1]==label] = global_tags[p_id]\n",
    "        \n",
    "        # update global tags\n",
    "        for (p_id, is_core), label in np_local:\n",
    "            if is_tagged[p_id]==1:\n",
    "                continue\n",
    "            global_tags[p_id] = label\n",
    "            is_tagged[p_id] = 1\n",
    "    return global_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RTree-based on boundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_cost(bounds, nPoints, fanout=2):\n",
    "    h = math.log((nPoints + 1) / fanout, fanout) + 1\n",
    "    DA = h + math.sqrt(nPoints) * 2 / (math.sqrt(fanout) -\n",
    "                                       1) + nPoints / (fanout - 1) + 1\n",
    "    return DA * nPoints\n",
    "\n",
    "def reduced_boundary_partition(rtree, maxPoints, eps):\n",
    "    mbr = rtree.bounds\n",
    "    partition_list = []\n",
    "    queue = deque()       \n",
    "    #Create queue: queue\n",
    "    queue.append(mbr)     \n",
    "    #In the right side of the queue, add the minimum limited matrix of MBR\n",
    "    while len(queue):     \n",
    "        br = queue.popleft()   \n",
    "        #获取最左边一个元素也就是第一个mbr=br，并在队列queue中删除\n",
    "        nPoints = rtree.count(br) \n",
    "        #Get the leftmost element, that is, the first MBR = BR, and delete it in the queue queue \n",
    "        if nPoints > maxPoints:   \n",
    "        #The number of sample points in the first MBR > the maximum number\n",
    "            (br1, br2) = _reduced_boundary_split(rtree, br, eps)\n",
    "            queue.append(br1)\n",
    "            queue.append(br2)\n",
    "        else:\n",
    "            partition_list.append(br)\n",
    "    return partition_list\n",
    "\n",
    "\n",
    "\n",
    "def _reduced_boundary_split(rtree, br, eps):\n",
    "\n",
    "    (xmin, ymin, xmax, ymax) = br           \n",
    "\n",
    "    #vertical splitline candidates\n",
    "    ymin_score = float('inf')               #Initialize to positive infinity\n",
    "    ysplit = ymin + (ymax - ymin) / 2       #Middle of y value\n",
    "    ybest_split = ((xmin, ymin, xmax, ysplit), (xmin, ysplit, xmax, ymax))  #The first time is to divide the vertical axis into two and a half\n",
    "    while (ysplit + eps * 2 <= ymax):       \n",
    "        br1 = (xmin, ymin, xmax, ysplit)    \n",
    "        br2 = (xmin, ysplit, xmax, ymax)    #Bisect the br rectangle on the vertical axis\n",
    "        point_diff = abs(rtree.count(br1) - rtree.count(br2)) #The difference of the number of sample points contained in the upper and lower bisection rectangle\n",
    "        score = point_diff * rtree.count((xmin, ysplit - eps, xmax, ysplit + eps))\n",
    "                                            \n",
    "        if score < ymin_score:              \n",
    "            ymin_score = score              \n",
    "            ybest_split = (br1, br2)        \n",
    "            if rtree.count(br1) > rtree.count(br2):   \n",
    "                ysplit = ymin + (ysplit - ymin) / 2  \n",
    "            else:\n",
    "                ysplit = ysplit + (ymax - ysplit) / 2\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #horizontal splitline candidates\n",
    "    xsplit = xmin + eps * 2\n",
    "    xmin_score = float('inf')\n",
    "    xbest_split = ((xmin, ymin, xsplit, ymax), (xsplit, ymin, xmax, ymax))\n",
    "    while (xsplit + eps * 2 <= xmax):\n",
    "        br1 = (xmin, ymin, xsplit, ymax)\n",
    "        br2 = (xsplit, ymin, xmax, ymax)\n",
    "        point_diff = abs(rtree.count(br1) - rtree.count(br2))\n",
    "        score = point_diff * rtree.count((xmin - eps, ymin, xmin + eps, ymax))\n",
    "        if score < xmin_score:\n",
    "            xmin_score = score\n",
    "            xbest_split = (br1, br2)\n",
    "            if rtree.count(br1) > rtree.count(br2):\n",
    "                xsplit = xmin + (xsplit - xmin) / 2\n",
    "            else:\n",
    "                xsplit = xsplit + (xmax - xsplit) / 2\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if xmin_score < ymin_score:\n",
    "        return xbest_split\n",
    "    else:\n",
    "        return ybest_split\n",
    "\n",
    "\n",
    "#construct rtree index\n",
    "def construct_rtree_index(dataset):\n",
    "    p = index.Property()\n",
    "    rtree_idx = index.Index(properties=p)\n",
    "    count = 0\n",
    "    for coordinate in dataset:\n",
    "        rtree_idx.insert(count, (*coordinate, *coordinate))\n",
    "        count += 1\n",
    "    return rtree_idx\n",
    "\n",
    "\n",
    "def rbs_fixnum(rtree, eps, n_partition):\n",
    "    mbr = rtree.bounds\n",
    "    partition_list = []\n",
    "    queue = PriorityQueue()\n",
    "    queue.put((-rtree.count(mbr), mbr))\n",
    "    while queue.qsize() < n_partition:\n",
    "        (score, br) = queue.get()\n",
    "        subbrs = _reduced_boundary_split(rtree, br, eps)\n",
    "        for subbr in subbrs:\n",
    "\n",
    "            queue.put((-rtree.count(subbr), subbr))\n",
    "    while queue.qsize() > 0:\n",
    "        (score, br) = queue.get()\n",
    "        partition_list.append(br)\n",
    "    return partition_list\n",
    "\n",
    "\n",
    "@timeit\n",
    "def rtree_partition(dataset, n_patitions, eps, mtd='rbs'):\n",
    "    \"\"\"\n",
    "    params:parameters needed for different spliting method; for cost-base method params=('cbs', max_cost);\n",
    "    for reduced-boundary method params = ('rbs', max_points)\n",
    "    \"\"\"\n",
    "    idx = construct_rtree_index(dataset)\n",
    "    \n",
    "    # #split test\n",
    "    if mtd == 'rbs':\n",
    "        partitioned = cbs_fixnum(idx, eps, n_patitions)\n",
    "    else:\n",
    "        partitioned = rbs_fixnum(idx, eps, n_patitions)\n",
    "\n",
    "    indexed_data = []\n",
    "    id_ptt = 0\n",
    "    for boundary in partitioned:\n",
    "        (left, bot, right, top) = boundary\n",
    "        for id_pts in idx.intersection((left - eps, bot - eps, right + eps, top + eps)):    \n",
    "#找出与(left - eps, bot - eps, right + eps, top + eps)有公共区域的所有的区域;计算这些点有已经存在的点的交点\n",
    "            indexed_data.append([id_ptt, id_pts])\n",
    "        id_ptt += 1\n",
    "\n",
    "    res = sc.parallelize(\n",
    "        indexed_data).groupByKey().map(lambda x: [x[0], list(x[1])])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entry Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(dataset, eps, partition_tuple, rtree=False, rtree_mtd='cbs'):\n",
    "    if rtree:\n",
    "        partition_tuple = np.prod(partition_tuple)\n",
    "        partitioned_rdd = rtree_partition(dataset,\n",
    "                                          partition_tuple,\n",
    "                                          eps,\n",
    "                                          mtd=rtree_mtd)\n",
    "    else:\n",
    "        partitioned_rdd = spatial_partition(dataset, partition_tuple, eps)\n",
    "    return partitioned_rdd\n",
    "\n",
    "\n",
    "# parallel entry function\n",
    "@timeit\n",
    "def parallel_dbscan(dataset,\n",
    "                    eps,\n",
    "                    min_pts,\n",
    "                    partition_tuple,\n",
    "                    method='matrix',\n",
    "                    metric='euclidian',\n",
    "                    rtree=False,\n",
    "                    rtree_mtd='cbs',\n",
    "                    verbose=False):\n",
    "\n",
    "    #     b_dataset = sc.broadcast(dataset)\n",
    "    #     b_eps = sc.broadcast(eps)\n",
    "    #     b_min_pts = sc.broadcast(min_pts)\n",
    "    partitioned_rdd = partition(dataset,\n",
    "                                eps,\n",
    "                                partition_tuple,\n",
    "                                rtree=rtree,\n",
    "                                rtree_mtd=rtree_mtd)\n",
    "    local_tags = partitioned_rdd.mapValues(lambda x: local_dbscan(\n",
    "        x, method=method, metric=metric)).collect()\n",
    "    result_tags = merge(local_tags, dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        p = partitioned_rdd.collect()\n",
    "        print('Partition info:')\n",
    "        for i in range(len(p)):\n",
    "            print('partition ', p[i][0],\n",
    "                  ', num of points: ', len(p[i][1]),\n",
    "                  ', num of clusters: ', len(np.unique(np.array(local_tags[i][-1])[:, 1])))\n",
    "        print('Number of Clusters:')\n",
    "\n",
    "    return result_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "def plot_result(dataset, result_tags, file_name=None, style='ticks'):\n",
    "    # data preparation\n",
    "    dataset = np.array(dataset)\n",
    "    df = pd.DataFrame()\n",
    "    df['x'] = dataset[:, 0]\n",
    "    df['y'] = dataset[:, 1]\n",
    "    df['label'] = result_tags\n",
    "    \n",
    "    # sns preparation\n",
    "#     sns.set_context(\"notebook\", font_scale=1)\n",
    "    sns.set_style(style)\n",
    "    f, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    \n",
    "    sns.scatterplot('x', 'y',data=df, ax=axes[0], palette='Blues_d')\n",
    "    sns.scatterplot('x', 'y', data=df, hue='label', ax=axes[1])\n",
    "    \n",
    "    plt.title(f'DBSCAN Result of {file_name}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "#     plt.legend(loc='upper right')\n",
    "    plt.gca().get_legend().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '.\\2.dataset\\jain.txt'\n",
    "dataset = np.array(load_data_label(test_file))\n",
    "n_partitions = (4, 2)\n",
    "eps = 2\n",
    "min_pts = 18\n",
    "\n",
    "mdbscan = MatrixDBSCAN(dataset, eps, min_pts)\n",
    "result_tags = mdbscan.predict()\n",
    "Evaluation.silhouette_coefficient(mdbscan)\n",
    "del mdbscan\n",
    "#plot_result(dataset, result_tags, 'Jain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  spiral_312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './2.dataset/spiral.txt'\n",
    "dataset = np.array(load_data_label(test_file))\n",
    "# n_partition_set = [(2, 2), [2, 4], [4, 4], [4, 8]]\n",
    "n_partition_set = [[2, 2]]\n",
    "eps = 3.5\n",
    "min_pts = 2\n",
    "b_dataset = sc.broadcast(dataset)\n",
    "b_eps = sc.broadcast(eps)\n",
    "b_min_pts = sc.broadcast(min_pts)\n",
    "\n",
    "for n_partitions in n_partition_set:\n",
    "    print(f'\\nPartitions: {np.prod(n_partitions)}')\n",
    "    print('\\nRtree-rbs:')\n",
    "    result_tags = parallel_dbscan(dataset, eps, min_pts, n_partitions, method='matrix', metric='fast-euclidian',\n",
    "                                 rtree=True, rtree_mtd='rbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdbscan = MatrixDBSCAN(dataset, eps, min_pts, metric='euclidian')\n",
    "result_tags = mdbscan.predict()\n",
    "plot_result(dataset, result_tags, 'Spiral')\n",
    "Evaluation.silhouette_coefficient(mdbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agg_788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './2.dataset/aggregation.txt'\n",
    "dataset = np.array(load_data_label(test_file))\n",
    "n_partition_set = [(2, 2), [2, 4], [4, 4]]\n",
    "# n_partition_set = [[2, 4], [4, 4], [4,8]]\n",
    "eps = 2.2\n",
    "min_pts = 23\n",
    "b_dataset = sc.broadcast(dataset)\n",
    "b_eps = sc.broadcast(eps)\n",
    "b_min_pts = sc.broadcast(min_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# serial naive\n",
    "print('\\nSERIAL:')\n",
    "mdbscan = NaiveDBSCAN(dataset, eps, min_pts, metric='euclidian')\n",
    "result_tags = mdbscan.predict()\n",
    "\n",
    "print('\\nSERIAL:')\n",
    "mdbscan = MatrixDBSCAN(dataset, eps, min_pts, metric='euclidian')\n",
    "result_tags = mdbscan.predict()\n",
    "Evaluation.silhouette_coefficient(mdbscan)\n",
    "\n",
    "for n_partitions in n_partition_set:\n",
    "    print(f'\\nPartitions: {np.prod(n_partitions)}')\n",
    "    print('\\nRtree-rbs:')\n",
    "    result_tags = parallel_dbscan(dataset, eps, min_pts, n_partitions, method='matrix', metric='fast-euclidian',\n",
    "                                 rtree=True, rtree_mtd='rbs')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 3\n",
    "min_pts = 34\n",
    "\n",
    "mdbscan = MatrixDBSCAN(dataset, eps, min_pts, metric='euclidian')\n",
    "result_tags = mdbscan.predict()\n",
    "print('Number of clusters:', len(np.unique(np.array(result_tags))))\n",
    "Evaluation.silhouette_coefficient(mdbscan)\n",
    "plot_result(dataset, result_tags, 'Aggregation_778')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
